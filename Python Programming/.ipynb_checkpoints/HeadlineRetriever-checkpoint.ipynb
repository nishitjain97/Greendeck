{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a python script to fetch headlines from: https://www.hindustantimes.com/\n",
    "Save them in a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Important modules\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup # For html parsing\n",
    "from requests import get # For retrieving web page\n",
    "import pandas as pd # To save into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending header with page request to mimic a user behaviour\n",
    "headers = ({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check robots.txt\n",
    "Robots.txt specifies whether we can scrape a web page or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_allowance(response):\n",
    "    \"\"\"\n",
    "        Goes line by line through robots.txt file. Checks if contains 'User-agent: *' and 'Disallow: /'.\n",
    "        This would mean disallowing everything to user agent, so it would be wrong to scrape homepage.\n",
    "    \"\"\"\n",
    "    lines = response.text.split('\\n')\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(lines):\n",
    "        if 'User-agent: *' in lines[i]:\n",
    "            i += 1\n",
    "            \n",
    "            while 'User-agent:' not in lines[i]:\n",
    "                if lines[i] == 'Disallow: /':\n",
    "                    return False\n",
    "                i += 1\n",
    "        i += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hindustantimes.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status for scraping https://www.hindustantimes.com/ : True\n"
     ]
    }
   ],
   "source": [
    "robot_response = get(url + 'robots.txt', headers=headers)\n",
    "print(\"Status for scraping \" + url + \" : \" + str(check_allowance(robot_response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Get the html code of homepage\n",
    "response = get(url, headers=headers)\n",
    "\n",
    "# Check the response status\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<html lang=\"en\">\r\n",
      "<head>\r\n",
      "<!-- don't make any changes here -->\r\n",
      "<!-- don't make any changes end here -->\r\n",
      "<title>Hindustan Times: Latest News, Breaking News and Today’s News Headlines | Hindustan Times</title>\r\n",
      "\t<meta name=\"Description\" content=\"Hindustan Times is India’s No.1 English News website where users can find Latest News, Breaking News, Today’s News Headlines, Trending News and updates from India and the World. Also watch latest photos and videos based on current affairs.\" />\r\n",
      "\r\n",
      "<meta name=\"keywords\" content=\"news, breaking news, Union budget news, latest news, news today, daily news, english news, election news, news updates, headlines, India news, politics news, sports news, entertainment news, business news, live cricket score, cricket news, education news\" />\r\n",
      "<!-- section pages updated /PortalConfig/Common/jpt/meta/meta-page.jpt-->\r\n",
      "\t<script type=\"application/ld+json\">\r\n",
      "\t\t\t\t{ \r\n",
      " \t\t\t\t\t\"@context\" : \"https://schema.org\", \r\n",
      " \t\t\t\t\t\"@type\": \"WebPage\", \r\n",
      " \t\t\t\t\t\"\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    # If positive response, view part of html code\n",
    "    print(response.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parse the page for content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    List of html tags and classes that contain headlines:\n",
    "    - div.newtop-block h1\n",
    "    - div.newtop-block h2\n",
    "    - div.media-conversations-section div.new-assembly-elections div.headingfour\n",
    "    - div.media-conversations-section div.new-video-news div.headingfour\n",
    "    - section.post-election-container div.headingfour\n",
    "    - section.post-election-container div.headingfive\n",
    "    - div.editor-pick-section div.headingfour\n",
    "    - div.ipl-hm-container div.headingfour\n",
    "    - div.ipl-hm-container div.headingfive\n",
    "    - div.photos-section div.headingfive\n",
    "    - div.random-news-section h3\n",
    "    - div.random-news-section div.para-txt\n",
    "    - div.opinion-section div.headingfour\n",
    "    - div.opinion-section div.headingfive\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_get_all_headlines(response):\n",
    "    \"\"\"\n",
    "        Gets headlines using hard-coded parsing calls\n",
    "    \"\"\"\n",
    "    \n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    # The top block\n",
    "    newtop_block = html_soup.find_all('div', class_='newtop-block')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in newtop_block.find_all('h1') if item.text != ''])\n",
    "    out_list.extend([item.text.strip() for item in newtop_block.find_all('h2') if item.text != ''])\n",
    "    \n",
    "    # Media conversations block\n",
    "    media_convo = html_soup.find_all('div', class_='media-conversations-section')[0]\n",
    "    new_ass = media_convo.find_all('div', class_='new-assembly-elections')[0]\n",
    "    new_vid = media_convo.find_all('div', class_='new-video-news')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in new_ass.find_all('div', class_='headingfour') if item.text != ''])\n",
    "    out_list.extend([item.text.strip() for item in new_vid.find_all('div', class_='headingfour') if item.text != ''])\n",
    "    \n",
    "    # Post election block\n",
    "    post_ele = html_soup.find_all('section', class_='post-election-container')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in post_ele.find_all('div', class_='headingfour') if item.text != ''])\n",
    "    out_list.extend([item.text.strip() for item in post_ele.find_all('div', class_='headingfive') if item.text != ''])\n",
    "    \n",
    "    # Editors pick\n",
    "    edit_pick = html_soup.find_all('div', class_='editor-pick-section')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in edit_pick.find_all('div', class_='headingfour') if item.text != ''])\n",
    "    \n",
    "    # Assembly elections\n",
    "    hm_cont = html_soup.find_all('div', class_='ipl-hm-container')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in hm_cont.find_all('div', class_='headingfour') if item.text != ''])\n",
    "    out_list.extend([item.text.strip() for item in hm_cont.find_all('div', class_='headingfive') if item.text != ''])\n",
    "    \n",
    "    # Photos section\n",
    "    photo_sect = html_soup.find_all('div', class_='photos-section')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in photo_sect.find_all('headingfive') if item.text != ''])\n",
    "    \n",
    "    # Random news\n",
    "    rand_news = html_soup.find_all('div', class_='random-news-section')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in rand_news.find_all('h3') if item.text != ''])\n",
    "    out_list.extend([item.text.strip() for item in rand_news.find_all('div', class_='para-txt') if item.text != ''])\n",
    "    \n",
    "    # Opinion section\n",
    "    op_sect = html_soup.find_all('div', class_='opinion-section')[0]\n",
    "    \n",
    "    out_list.extend([item.text.strip() for item in op_sect.find_all('div', class_='headingfour') if item.text != ''])\n",
    "    out_list.extend([item.text.strip() for item in op_sect.find_all('div', class_='headingfive') if item.text != ''])\n",
    "    \n",
    "    # Remove any duplicate items\n",
    "    out_list = list(set(out_list))\n",
    "    \n",
    "    # Create python dataframe\n",
    "    series = pd.Series(out_list, index=range(len(out_list)), name='Headlines')\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_to_parse = [\n",
    "    'div.newtop-block h1',\n",
    "    'div.newtop-block h2',\n",
    "    'div.media-conversations-section div.new-assembly-elections div.headingfour',\n",
    "    'div.media-conversations-section div.new-video-news div.headingfour',\n",
    "    'section.post-election-container div.headingfour',\n",
    "    'section.post-election-container div.headingfive',\n",
    "    'div.editor-pick-section div.headingfour',\n",
    "    'div.ipl-hm-container div.headingfour',\n",
    "    'div.ipl-hm-container div.headingfive',\n",
    "    'div.photos-section div.headingfive',\n",
    "    'div.random-news-section h3',\n",
    "    'div.random-news-section div.para-txt',\n",
    "    'div.opinion-section div.headingfour',\n",
    "    'div.opinion-section div.headingfive'\n",
    "]\n",
    "\n",
    "def looped_get_all_headlines(response, elements_to_parse):\n",
    "    \"\"\"\n",
    "        Gets headlines using looped parsing calls.\n",
    "        Inputs:\n",
    "            - response: The returned value of a requests.get() call\n",
    "            - elements_to_parse: A list of html tags in the form 'tag.class inner-tag.class' that need to be parsed\n",
    "        \n",
    "        Output: Pandas Series object containing all headlines\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parser\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for element in elements_to_parse:\n",
    "        tag_hierarchy = element.split(' ') # Get the individual tags in the hierarchy\n",
    "        \n",
    "        html_block = html_soup # Make a duplicate of parser\n",
    "        \n",
    "        for tag_and_class in tag_hierarchy[:-1]:\n",
    "            # Keep pointing the parser to the next element in hierarchy until the second-last element\n",
    "            \n",
    "            try:\n",
    "                tag, class_ = tag_and_class.split('.')\n",
    "            except:\n",
    "                # In case there is no class for a tag\n",
    "                tag, class_ = tag_and_class, ''\n",
    "                \n",
    "            try:\n",
    "                # Find the first occurance of the tag\n",
    "                html_block = html_block.find_all(tag, class_=class_)[0]\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # Get the last element in tag hierarchy\n",
    "        try:\n",
    "            tag, class_ = tag_hierarchy[-1].split('.')\n",
    "        except:\n",
    "            tag, class_ = tag_hierarchy[-1], ''\n",
    "        \n",
    "        # Add the text contained in each element to the output list\n",
    "        out_list.extend([item.text.strip() for item in html_block.find_all(tag, class_=class_) if item.text != ''])\n",
    "    \n",
    "    # Remove duplicate elements\n",
    "    out_list = list(set(out_list))\n",
    "    \n",
    "    # Convert to Series object, sort and reindex\n",
    "    series = pd.Series(out_list, name='Headlines')\n",
    "    series = series.sort_values()\n",
    "    series.index = range(len(series))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. View and save headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = looped_get_all_headlines(response, elements_to_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10 yrs since 26/11, gaps still exist in securi...\n",
       "1    11,000 new polling booths in MP, highest among...\n",
       "2    2018 assembly polls fight against casteism, dy...\n",
       "3    25 killed in suicide bomb attack in market in ...\n",
       "4       26/11 case: Bizarre trial and a saga of delays\n",
       "Name: Headlines, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View top of list\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "headlines.to_csv('HTHeadlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
